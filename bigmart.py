# -*- coding: utf-8 -*-
"""BigMart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZB4oQBUPjbuoHw9Nl-EEksDvSI6vQuFU

# Product-Sales-Outlet-at-BigMart-Datawarehouse-

he data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim of this data science project is to build a predictive model and find out the sales of each product at a particular store.

# Data Loading and Import modules
"""

import numpy as np
import pandas as pd
import seaborn as sns
import warnings 
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

# Importing dataset and examining it

Test_data = pd.read_csv("/content/Test.csv", encoding="latin-1")
Train_data = pd.read_csv("/content/Train.csv", encoding="latin-1")
data = pd.concat([Test_data, Train_data], ignore_index=True) # linking (put together) the dataser Test and Train
data.head()

"""# Data Understanding

Data
We have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.

Variable - Description

Item_Identifier- Unique product ID

Item_Weight- Weight of product

Item_Fat_Content - Whether the product is low fat or not

Item_Visibility - The % of total display area of all products in a store allocated to the particular product

Item_Type - The category to which the product belongs

Item_MRP - Maximum Retail Price (list price) of the product

Outlet_Identifier - Unique store ID

Outlet_Establishment_Year- The year in which store was established

Outlet_Size - The size of the store in terms of ground area covered

Outlet_Location_Type- The type of city in which the store is located

Outlet_Type- Whether the outlet is just a grocery store or some sort of supermarket

Item_Outlet_Sales - Sales of the product in the particulat store. This is the outcome variable to be predicted.
"""

#checking the unique values
data.apply(lambda x: len(x.unique()))

#checking the data type
print(data.info())

#checking only the categore object 
Categore = []
for x in data.dtypes.index:
    if data.dtypes[x] == "object":
      Categore.append(x)
Categore

#checking only the categore float64 
Num = []
for x in data.dtypes.index:
    if data.dtypes[x] == "float64":
      Num.append(x)
Num

#Checking the statistical information
print (data.describe())

data.head()

#checking missing values  ITem_weight contains 2439 missing values, Outlet_Size contains 4016 missing values and Item_Outlet_Sales 5681 missing values
data.isnull().sum()

"""# Cleaning Data"""

#Removing columns that I do not need 
Categore.remove('Item_Identifier')
Categore.remove('Outlet_Identifier')
Categore

# check categorical variables Item_Fat_Content
data.Item_Fat_Content.value_counts()

# check categorical variables Item_Type
data.Item_Type.value_counts()

# check categorical variables Outlet_Size
data.Outlet_Size.value_counts()

# check categorical variables Outlet_Location_Type
data.Outlet_Location_Type.value_counts()

# check categorical variables Outlet_Type
data.Outlet_Type.value_counts()

data.Item_Weight.value_counts()

data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF': 'Low Fat','low fat': 'Low Fat', 'reg': 'Regular'})
data['Item_Fat_Content'].head(5)

data.Item_Fat_Content.value_counts()

"""Fill The missing values

For categorical variable object we have to mode

For categorical variable numerical we have to mean
"""

# average weight per item
item_av_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')
item_av_weight.head()
# create a mask of the missing data in Item_Weight
null_mask_weight = data['Item_Weight'].isnull()
# impute values
data.loc[null_mask_weight, 'Item_Weight'] = data.loc[null_mask_weight, 'Item_Identifier'].apply(lambda x: item_av_weight.loc[x])

#data['Outlet_Size']=data['Outlet_Size'].fillna('Unknown')  fill the date if you dont know the information, but u can also do a calculation using mode

o_s_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))
o_s_mode

fill_miss = data['Outlet_Size'].isnull()
data.loc[fill_miss, 'Outlet_Size'] = data.loc[fill_miss, 'Outlet_Type'].apply(lambda x: o_s_mode[x])

# average weight per item
Item_Outlet_Sales_av = data.pivot_table(values='Item_Outlet_Sales', index='Item_Identifier')
Item_Outlet_Sales_av.head()
# create a mask of the missing data in Item_Weight
null_mask_sales = data['Item_Outlet_Sales'].isnull()
# impute values
data.loc[null_mask_sales, 'Item_Outlet_Sales'] = data.loc[null_mask_sales, 'Item_Identifier'].apply(lambda x: Item_Outlet_Sales_av.loc[x])

data.isnull().sum()

sum(data['Item_Visibility']==0)

# replace zeros with mean
data.loc[:, 'Item_Visibility'].replace([0], [data['Item_Visibility'].mean()], inplace=True)

sum(data['Item_Visibility']==0)

data.head()

# create new attributes using the object Item_Identifier

data['New_Item_Type'] = data['Item_Identifier'].apply(lambda x: x[:2]) # using lambda to get the first 2 letter from Item_Identifier
data['New_Item_Type']

# combine item fat content
data ['New_Item_Type'] = data['New_Item_Type'].replace({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})
data['New_Item_Type'].value_counts()

# create small values for year
data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']
data['Outlet_Years']

data.loc[data['New_Item_Type']=='Non-Consumable', 'Item_Fat_Content'] = 'Not Food'
data['Item_Fat_Content'].value_counts()

"""# Data Analysis"""

# Showing the corretaion between variables

# Positive correlation between the variables 'Item_MRP' and 'Item_Outlet_Sales'.
# Negative correlation between the variables 'Outlet_Years' from 'Oulet_Establishment_Year'
corr = data.corr()
sns.heatmap(corr, annot=True, cmap= sns.diverging_palette(20, 220, n=200),)

sns.set_theme(style="dark")
sns.countplot(data["Item_Fat_Content"])

#We can see that most things are low-fat substance.

sns.set_theme(style="dark")
sns.countplot(data["New_Item_Type"])
#We can see that most things are Food substance.

sns.set_theme(style="dark")
lista = list(data['Item_Type'].unique())
plt.figure(figsize = (20, 10))
chart=sns.countplot(data["Item_Type"])
chart.set_xticklabels(labels=lista, rotation=50)

sns.set_theme(style="dark")
sns.countplot(data["Outlet_Size"])

sns.set_theme(style="dark")
sns.countplot(data["Outlet_Location_Type"])

sns.set_theme(style="dark")
plt.figure(figsize = (10, 8))
sns.countplot(data["Outlet_Type"])

sns.set_theme(style="dark")
plt.figure(figsize = (10, 8))
sns.countplot(data['Outlet_Establishment_Year'])

ax = data.hist(column='Item_MRP' , by='Outlet_Type', bins=100, density=True)

"""# Model 

to do a model I need to convert the categorical column into the numerical column.
"""

# to do a model I need to convert the categorical column into the numerical column.
from sklearn.preprocessing import LabelEncoder

l = LabelEncoder()
data['Outlet'] = l.fit_transform(data['Outlet_Identifier'])
catego = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type']
for colu in catego:
    data[colu] = l.fit_transform(data[colu])

data.head()

#Splitting the data for Training and Testing
X = data.drop(columns=['Outlet_Establishment_Year', 'Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'])
y = data['Item_Outlet_Sales']

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error

def train(modelo, X, y):
    # train the model
    modelo.fit(X, y)
    
    # predict the training set
    predit = modelo.predict(X)
    
    # perform cross-validation
    cross_validation_score = cross_val_score(modelo, X, y, scoring='neg_mean_squared_error', cv=5)
    cross_validation_score  = np.abs(np.mean(cross_validation_score ))
    
    # X contains input attributes and y contains the output attribute. We use 'cross val score()' for better validation of the model.

    #Here, cv=5 means that the cross-validation will split the data into 5 parts. 

    #np.abs() will convert the negative score to positive and np.mean() will give the average value of 5 scores.
    
    print("Model Report")
    print("MSE:",mean_squared_error(y,predit))
    print("cross validation score :", cross_validation_score )

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso

modelo = LinearRegression(normalize=True)
train(modelo, X, y)
amount = pd.Series(modelo.coef_, X.columns).sort_values()
amount.plot(kind='bar', title="Model Coefficients")

"""# Lasso

"""

from sklearn.linear_model import Lasso

modelo = Lasso(alpha=0.01, normalize=True)
train(modelo, X, y)

# visualise result
amount = pd.Series(modelo.coef_, X.columns).sort_values()
amount.plot(kind='bar', title='Model Coefficients')

"""# Decision Tree Regressor"""

from sklearn.tree import DecisionTreeRegressor

modelo = DecisionTreeRegressor()
train(modelo, X, y)
amount = pd.Series(modelo.feature_importances_, X.columns).sort_values(ascending=False)
amount.plot(kind='bar', title="Feature Importance")

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
modelo = RandomForestRegressor()
train(modelo, X, y)
amount = pd.Series(modelo.feature_importances_, X.columns).sort_values(ascending=False)
amount.plot(kind='bar', title="Feature Importance")

"""# Ridge"""

modelo = Ridge(normalize=True)
train(modelo, X, y)
amount = pd.Series(modelo.coef_, X.columns).sort_values()
amount.plot(kind='bar', title="Model Coefficients")